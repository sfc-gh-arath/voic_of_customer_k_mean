{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "gqun52qa7gotdrahm7xk",
   "authorId": "330324676895",
   "authorName": "ADMIN",
   "authorEmail": "aswinee.rath@snowflake.com",
   "sessionId": "e65cc25f-90e6-4f56-bf42-737c0ae5aba9",
   "lastEditTime": 1764696403668
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "import_packages",
    "codeCollapsed": false
   },
   "source": "import os, json, math, textwrap, ast\nimport numpy as np\nimport pandas as pd\nimport streamlit as st\n\n# # Optional GPU accel; safe to ignore if not available\n# try:\n#     import cuml.accel\n#     cuml.accel.install()\n#     print(\"RAPIDS acceleration enabled.\")\n# except Exception as e:\n#     print(\"RAPIDS accel not available; CPU mode.\", e)\n\n##from umap import UMAP\n##from hdbscan import HDBSCAN\n\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.core import Root\n\nsession = get_active_session()\nroot = Root(session)\n\nprint(\"âœ… Snowpark session ready.\")\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "990f9a22-960e-42f4-a4ea-2a68b68f82a0",
   "metadata": {
    "language": "python",
    "name": "vars"
   },
   "outputs": [],
   "source": "# ==== SOURCE DATA ====\n# Expected schema (minimum): SURVEY_ID STRING, KEYWORDS STRING(JSON of {\"keywords\": [\"...\",\"...\"]})\nSOURCE_TABLE = \"SURVEY_KEY_TOPICS\"   # <- change if different\n\n# ==== INTERMEDIATE / OUTPUT TABLES ====\nSURVEY_KEYWORDS_EXPLODED = \"SURVEY_KEYWORDS_EXPLODED\"      # SURVEY_ID, KEYWORD\nKEYWORD_EMBEDDINGS       = \"KEYWORD_EMBEDDINGS\"            # KEYWORD, EMBEDDING (ARRAY/FLOAT)\nKEYWORD_CLUSTERS         = \"KEYWORD_CLUSTERS\"              # KEYWORD, CLUSTER_ID, CLUSTER_LABEL\nCLUSTER_DICTIONARY       = \"CLUSTER_DICTIONARY\"            # CLUSTER_ID, CLUSTER_LABEL, EXAMPLE_KEYWORDS, CLUSTER_SIZE\nSURVEY_CLUSTER_OUTPUT    = \"SURVEY_CLUSTER_OUTPUT\"         # SURVEY_ID, KEYWORDS (original), CLUSTER_LABELS (ARRAY)\n\n# ==== EMBEDDING ====\nEMBED_MODEL      = \"snowflake-arctic-embed-m-v1.5\"\nEMBED_BATCH_SIZE = 256       # Safe batch size; tune if you like\nEMBED_DTYPE_NP   = np.float32\n\n# ==== CLUSTERING ====\n# We keep it simple: HDBSCAN on raw embeddings (UMAP optional). HDBSCAN finds K automatically.\nUSE_UMAP              = False  # Set True for nicer separation on large corpora\nUMAP_N_COMPONENTS     = 5\nUMAP_N_NEIGHBORS      = 15\nUMAP_MIN_DIST         = 0.0\nUMAP_METRIC           = \"cosine\"\nHDBSCAN_MIN_CLUSTER_SIZE = None  # if None -> heuristic sqrt(n)\nHDBSCAN_MIN_SAMPLES      = None  # let HDBSCAN infer\nHDBSCAN_METRIC           = \"euclidean\"  # on UMAP/embedding space\nOUTLIER_LABEL            = \"Other\"      # label for cluster -1\n\n# ==== LABELING ====\nLABEL_MODEL       = \"snowflake-llama-3.3-70b\"\nMAX_LABEL_LEN     = 60\nLABEL_TOP_K_WORDS = 12        # how many keywords to show the model per cluster\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4934b77e-6024-4a3c-ba28-2cd1faca9493",
   "metadata": {
    "language": "sql",
    "name": "check_PRODUCT_SUVEY_data"
   },
   "outputs": [],
   "source": "-- pre-uploaded synthetic data\nSELECT *\nFROM DEMO_DB.CRM.PRODUCT_SURVEY\nLIMIT 10;\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "51f12d64-6eb5-4f4e-97d5-6115bf923bb4",
   "metadata": {
    "name": "Extract_and_Embed_Keywords",
    "collapsed": false
   },
   "source": "## Extract and Embed Keywords/Aspects"
  },
  {
   "cell_type": "code",
   "id": "46167e69-0bf5-4ff9-a7f5-932d6b1ca39c",
   "metadata": {
    "language": "sql",
    "name": "extract_aspects",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "create or replace table SURVEY_KEY_TOPICS\nas\nSELECT r.survey_id, r.review_text,\nAI_COMPLETE(\nMODEL => 'CLAUDE-4-SONNET',\nPROMPT => 'Extract 1-6 concise product aspects from this retail review.\\n' ||\n'Rules: short phrases (2-4 words), copy from text if possible, no sentiment words, no duplicates.\\n' ||\n'Example: Great quality-husband loves them. Really comfy and true to size. -> [\"quality\", \"comfy\", \"size\"]\\n' ||\n'Return ONLY a JSON array of strings.\\n\\n' || r.REVIEW_TEXT,\n response_format => {\n    'type': 'json',\n    'schema': {\n      'type': 'object',\n      'properties': {\n        'keywords': {\n          'type': 'array',\n          'items': {'type': 'string'}\n            }\n        }\n    }\n}\n,\nMODEL_PARAMETERS => {\n        'temperature': 0,\n        'max_tokens': 4096\n    }\n) as KEYWORDS\nFROM DEMO_DB.CRM.PRODUCT_SURVEY r\nwhere 1=1\nand REVIEW_TEXT != ''\n--limit 5\n;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6fddd34b-b6a7-4249-80cb-ef6fd6a559cc",
   "metadata": {
    "language": "sql",
    "name": "extracted_aspects"
   },
   "outputs": [],
   "source": "SELECT * FROM SURVEY_KEY_TOPICS limit 10;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bff5eb3b-e383-4df8-af6c-fecbc65cedc4",
   "metadata": {
    "language": "sql",
    "name": "vector_embed_aspects"
   },
   "outputs": [],
   "source": "-- 3A) Start from a clean exploded table (no embeddings here)\nCREATE OR REPLACE TABLE SURVEY_KEYWORDS_EXPLODED AS\nSELECT\n  s.SURVEY_ID,\n  LOWER(TRIM(f.value::string)) AS KEYWORD\nFROM SURVEY_KEY_TOPICS s,\nLATERAL FLATTEN( INPUT => TRY_PARSE_JSON(s.KEYWORDS):keywords ) f\nWHERE f.value IS NOT NULL\n  AND TRY_PARSE_JSON(s.KEYWORDS) IS NOT NULL\n  AND TRIM(f.value::string) <> '';\n\n-- 3B) Make a DEDUPED list of unique keywords\nCREATE OR REPLACE TEMP TABLE _UNIQUE_KEYWORDS AS\nSELECT DISTINCT KEYWORD\nFROM SURVEY_KEYWORDS_EXPLODED\nWHERE KEYWORD IS NOT NULL AND KEYWORD <> '';\n\n-- 3C) Embed ONCE per unique keyword \nCREATE OR REPLACE TABLE KEYWORD_EMBEDDINGS AS\nSELECT\n  KEYWORD,\n  AI_EMBED('snowflake-arctic-embed-m-v1.5', KEYWORD) AS EMBEDDING\nFROM _UNIQUE_KEYWORDS;\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b625fb21-2530-4078-ac04-3023ee3bac81",
   "metadata": {
    "language": "sql",
    "name": "preview_emebeddings"
   },
   "outputs": [],
   "source": "select * from KEYWORD_EMBEDDINGS order by keyword desc limit 5;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ea918f81-35b5-4c53-879d-5119579fd310",
   "metadata": {
    "name": "Cluster_Analysis",
    "collapsed": false
   },
   "source": "## Cluster Analysis"
  },
  {
   "cell_type": "code",
   "id": "c9edb806-29b0-4a32-9b4a-c15d3d141c1a",
   "metadata": {
    "language": "python",
    "name": "create_x_for_cluster_analysis"
   },
   "outputs": [],
   "source": "# Pull a deterministic, unique keyword list for clustering\nkw_df = session.table(\"KEYWORD_EMBEDDINGS\") \\\n               .select(\"KEYWORD\", \"EMBEDDING\") \\\n               .sort(\"KEYWORD\") \\\n               .to_pandas()\n\n# Build a KEYWORD -> vector map (ensure float32)\nembed_map = {\n    row[\"KEYWORD\"]: np.asarray(row[\"EMBEDDING\"], dtype=np.float32)\n    for _, row in kw_df.iterrows()\n}\n\n# The ordered keyword list used for clustering\nkeywords = kw_df[\"KEYWORD\"].tolist()\n\n# Build X in the SAME order\nX = np.vstack([embed_map[k] for k in keywords]).astype(np.float32)\n\nprint(\"âœ… Embedding matrix X:\", X.shape, \"| unique keywords:\", len(keywords))\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40d87aab-4d0c-473e-85b2-cbf2d12aa91b",
   "metadata": {
    "language": "python",
    "name": "clusters"
   },
   "outputs": [],
   "source": "from sklearn.preprocessing import normalize\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# X, keywords are already built in Cell 3\nn = len(keywords)\nassert X.shape[0] == n, \"X and keywords length mismatch.\"\n\n# 1) L2-normalize (spherical k-means behavior; cosine-friendly)\nX_norm = normalize(X.astype(np.float32), norm=\"l2\", axis=1)\n\n# 2) (Optional) PCA to denoise high-dim vectors when n is small\n#    Keep enough components but not more than n-1 to avoid degenerate cases\nUSE_PCA = True\nPCA_COMPONENTS = min(100, max(10, n - 1))  # between 10 and 100, capped by n-1\n\nif USE_PCA:\n    pca = PCA(n_components=PCA_COMPONENTS, random_state=42)\n    X_work = pca.fit_transform(X_norm)\nelse:\n    X_work = X_norm\n\n# 3) Choose K automatically by silhouette over a small, sensible range\ndef choose_k(X_mat, k_min=4, k_max=12):\n    ks = []\n    scores = []\n    for k in range(k_min, min(k_max, max(2, n - 1)) + 1):\n        km = KMeans(n_clusters=k, random_state=42, n_init=\"auto\")\n        labels = km.fit_predict(X_mat)\n        # cosine silhouette pairs well with normalized embeddings; for PCA space euclidean is fine too\n        # If you used PCA, euclidean silhouette is consistent with KMeans objective:\n        metric = \"euclidean\" if USE_PCA else \"cosine\"\n        # Handle tiny clusters edge cases\n        if len(set(labels)) < 2:\n            continue\n        try:\n            score = silhouette_score(X_mat, labels, metric=metric)\n            ks.append(k)\n            scores.append(score)\n        except Exception:\n            # Can fail for pathological clusterings; skip\n            pass\n    if not scores:\n        # Fallback: 5 clusters if nothing scored\n        return 5, None\n    best_idx = int(np.argmax(scores))\n    return ks[best_idx], float(scores[best_idx])\n\nk_opt, sil = choose_k(X_work, k_min=6, k_max=10)\nprint(f\"ðŸ”Ž Auto-selected K={k_opt} (silhouette={sil if sil is not None else 'n/a'})\")\n\n# 4) Final KMeans fit\nkmeans = KMeans(n_clusters=k_opt, random_state=42, n_init=\"auto\")\nlabels = kmeans.fit_predict(X_work)\n\n# (Optional) get centroids back in original space for reference/labeling\n# For PCA case, map centroids back to normalized space (approximate)\nif USE_PCA:\n    centroids_pca = kmeans.cluster_centers_\n    centroids_norm = pca.inverse_transform(centroids_pca)\nelse:\n    centroids_norm = kmeans.cluster_centers_\n\n# Ensure centroids are unit-norm (useful for â€œnearest keywordâ€ medoids)\ncentroids_norm = normalize(centroids_norm, norm=\"l2\", axis=1)\n\n# Metrics\nn_clusters = len(set(labels))\nsizes = pd.Series(labels).value_counts().sort_index()\nprint(f\"ðŸ“Š KMeans clusters: {n_clusters}\")\nprint(\"Cluster sizes:\\n\", sizes.to_string())\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e0458e51-5b95-4efa-95fc-4c47171acf55",
   "metadata": {
    "language": "python",
    "name": "tighten_clusters",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import normalize\n\n# Inputs available from prior cells:\n# - X (n x d) or X_work; we'll use X_norm for cosine sims (unit vectors)\n# - labels: np.ndarray of shape (n,)\n# - keywords: list[str] length n\n\n# If you don't already have X_norm, create it:\ntry:\n    X_norm  # noqa: F401\nexcept NameError:\n    X_norm = normalize(X.astype(np.float32), norm=\"l2\", axis=1)\n\n# --- knobs ---\nMIN_SIM_TO_MERGE = 0.75  # if singleton's best cosine to any centroid >= this, merge; else keep singleton\n\n# Helper: compute unit centroids per cluster\ndef centroids_from_labels(Xu, lbls):\n    centroids = {}\n    members = {}\n    for cid in np.unique(lbls):\n        idxs = np.where(lbls == cid)[0]\n        members[int(cid)] = idxs\n        c = Xu[idxs].mean(axis=0).astype(np.float32)\n        c /= (np.linalg.norm(c) + 1e-12)\n        centroids[int(cid)] = c\n    return centroids, members\n\nlabels_before = labels.copy()\nsizes_before = pd.Series(labels_before).value_counts().sort_index()\nprint(\"Before (sizes):\\n\", sizes_before.to_string())\n\n# Compute centroids and find singleton clusters\nC, members = centroids_from_labels(X_norm, labels_before)\nsingleton_ids = [cid for cid, idxs in members.items() if len(idxs) == 1]\n\nif singleton_ids:\n    # Build matrix of non-singleton centroids\n    non_singletons = [cid for cid, idxs in members.items() if len(idxs) > 1]\n    if non_singletons:  # only proceed if there is at least one non-singleton to merge into\n        M = np.stack([C[cid] for cid in non_singletons], axis=0)  # rows are unit vectors\n        M_ids = non_singletons\n\n        labels_after = labels_before.copy()\n        for cid in singleton_ids:\n            idx = members[cid][0]        # the single member's row index\n            v = X_norm[idx]              # unit vector for that keyword\n            sims = M @ v                 # cosine similarity to each non-singleton centroid\n            j = int(np.argmax(sims))\n            if sims[j] >= MIN_SIM_TO_MERGE:\n                # merge singleton into the nearest non-singleton cluster\n                labels_after[idx] = M_ids[j]\n            # else: keep as its own tiny cluster\n\n    else:\n        labels_after = labels_before  # nothing to merge into\nelse:\n    labels_after = labels_before      # no singletons\n\nsizes_after = pd.Series(labels_after).value_counts().sort_index()\nprint(\"\\nAfter singleton-merge (sizes):\\n\", sizes_after.to_string())\n\n# Use these going forward\nfinal_labels = labels_after ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "97b56565-55b0-4def-9126-1d990be3eadd",
   "metadata": {
    "language": "python",
    "name": "cluster_dictionary"
   },
   "outputs": [],
   "source": "# Keyword -> cluster dataframe\nkw_clusters = pd.DataFrame({\"KEYWORD\": keywords, \"CLUSTER_ID\": final_labels})\n\n# For each cluster, pick top K example keywords (by frequency in corpus)\n# Here all keywords are unique strings; so we just take any K per cluster.\ncluster_examples = (\n    kw_clusters[kw_clusters[\"CLUSTER_ID\"] != -1]\n    .groupby(\"CLUSTER_ID\")[\"KEYWORD\"]\n    .apply(lambda s: sorted(s.tolist())[:LABEL_TOP_K_WORDS])\n    .reset_index(name=\"EXAMPLE_KEYWORDS\")\n)\n\ncluster_sizes = (\n    kw_clusters[kw_clusters[\"CLUSTER_ID\"] != -1]\n    .groupby(\"CLUSTER_ID\")[\"KEYWORD\"]\n    .size()\n    .reset_index(name=\"CLUSTER_SIZE\")\n)\n\ncluster_dict = cluster_examples.merge(cluster_sizes, on=\"CLUSTER_ID\", how=\"left\")\nprint(f\"Will label {len(cluster_dict)} clusters (excluding outliers).\")\ncluster_dict",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56c1c3b1-0eb1-43ad-bece-6ac6ea74bd8c",
   "metadata": {
    "language": "python",
    "name": "assign_labels_to_clusters"
   },
   "outputs": [],
   "source": "if not cluster_dict.empty:\n    # Copy & prep\n    tmp = cluster_dict.copy()\n\n    # ðŸ”§ Remove any leftover label columns from previous runs (avoids *_x, *_y growth)\n    tmp = tmp.drop(columns=[c for c in tmp.columns if c.upper().startswith(\"CLUSTER_LABEL\")], errors=\"ignore\")\n\n    # Build prompt inputs\n    tmp[\"EXAMPLE_KEYWORDS_STR\"] = tmp[\"EXAMPLE_KEYWORDS\"].apply(lambda xs: \", \".join(xs))\n\n    # Overwrite temp table cleanly\n    session.sql(\"DROP TABLE IF EXISTS TMP_CLUSTER_DICT\").collect()\n    session.write_pandas(\n        tmp[[\"CLUSTER_ID\", \"EXAMPLE_KEYWORDS_STR\", \"CLUSTER_SIZE\"]],\n        \"TMP_CLUSTER_DICT\",\n        auto_create_table=True,\n        overwrite=True,\n    )\n\n    # Label clusters with Cortex (or AI_COMPLETE)\n    labeling_sql = f\"\"\"\n    WITH P AS (\n      SELECT \n        CLUSTER_ID,\n        CONCAT(\n          'You are a product quality assurance expert tasked with providing a concise label ',\n          'to group feedback categories from reviews for lululemon clothing.',\n          ' Given these example keywords: ',\n          EXAMPLE_KEYWORDS_STR,\n          ' â€” return a concise 1â€“3 word noun phrase that best names the shared theme/review topic.',\n          ' Return ONLY the phrase, no punctuation or quotes.'\n        ) AS PROMPT\n      FROM TMP_CLUSTER_DICT\n    )\n    SELECT \n      CLUSTER_ID,\n      AI_COMPLETE('{LABEL_MODEL}', PROMPT) AS RAW_LABEL\n    FROM P\n    \"\"\"\n    label_df = session.sql(labeling_sql).to_pandas()\n\n    def clean_label(s: str) -> str:\n        s = (s or \"\").strip().replace('\"', \"\").replace(\"'\", \"\")\n        s = s.splitlines()[0].strip()\n        return s[:MAX_LABEL_LEN] if len(s) > MAX_LABEL_LEN else s\n\n    label_df[\"CLUSTER_LABEL\"] = label_df[\"RAW_LABEL\"].map(clean_label)\n    label_df = label_df[[\"CLUSTER_ID\", \"CLUSTER_LABEL\"]]\n\n    # Merge back (now there's only ONE CLUSTER_LABEL)\n    cluster_dict = tmp.merge(label_df, on=\"CLUSTER_ID\", how=\"left\")\nelse:\n    cluster_dict[\"CLUSTER_LABEL\"] = []\n\n# Outlier row\noutlier_row = pd.DataFrame([{\n    \"CLUSTER_ID\": -1,\n    \"EXAMPLE_KEYWORDS\": [],\n    \"CLUSTER_SIZE\": int((labels == -1).sum()),\n    \"CLUSTER_LABEL\": OUTLIER_LABEL,\n}])\ncluster_dict_full = pd.concat([cluster_dict, outlier_row], ignore_index=True)\n\n# âœ… Normalize schema before write (prevents stray/suffixed columns)\ncluster_dict_full = cluster_dict_full[[\"CLUSTER_ID\", \"CLUSTER_LABEL\", \"EXAMPLE_KEYWORDS\", \"CLUSTER_SIZE\"]]\n\n# Overwrite the destination table\nsession.sql(f\"DROP TABLE IF EXISTS {CLUSTER_DICTIONARY}\").collect()\nsession.write_pandas(\n    cluster_dict_full.assign(\n        EXAMPLE_KEYWORDS=lambda df: df[\"EXAMPLE_KEYWORDS\"].apply(lambda xs: xs if isinstance(xs, list) else [])\n    ),\n    CLUSTER_DICTIONARY,\n    auto_create_table=True,\n    overwrite=True,\n)\n\nprint(\"âœ… Cluster dictionary & labels\")\ncluster_dict_full\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6ce373e6-7263-47ac-b5e6-f7390a67d118",
   "metadata": {
    "language": "python",
    "name": "map_keywords_to_cluster_labels"
   },
   "outputs": [],
   "source": "kw_clusters = kw_clusters.merge(cluster_dict_full[[\"CLUSTER_ID\",\"CLUSTER_LABEL\"]], on=\"CLUSTER_ID\", how=\"left\")\n\nsession.write_pandas(kw_clusters, KEYWORD_CLUSTERS, auto_create_table=True, overwrite=True)\nprint(\"âœ… Keyword cluster assignments written.\")\nkw_clusters\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "62193f53-4af1-41bd-83ac-658bf1b8cf24",
   "metadata": {
    "language": "python",
    "name": "map_to_cluster_labels_and_keywords_to_survey"
   },
   "outputs": [],
   "source": "# 1) Bring original KEYWORDS JSON from source table\nsrc = session.table(SOURCE_TABLE).select(\"SURVEY_ID\", \"KEYWORDS\")\n\n# 2) Join exploded keywords -> cluster labels\nexploded = session.table(SURVEY_KEYWORDS_EXPLODED).select(\"SURVEY_ID\",\"KEYWORD\")\nkwc = session.table(KEYWORD_CLUSTERS).select(\"KEYWORD\",\"CLUSTER_LABEL\")\n\n# 3) Aggregate deduped labels per SURVEY_ID\nsession.sql(f\"\"\"\nCREATE OR REPLACE TEMP VIEW V_SURVEY_LABELS AS\nSELECT\n  e.SURVEY_ID,\n  ARRAY_DISTINCT(ARRAY_AGG(kc.CLUSTER_LABEL)) AS CLUSTER_LABELS\nFROM {SURVEY_KEYWORDS_EXPLODED} e\nJOIN {KEYWORD_CLUSTERS} kc\n  ON e.KEYWORD = kc.KEYWORD\nGROUP BY e.SURVEY_ID\n\"\"\").collect()\n\n# 4) Build the output table exactly as requested\nsession.sql(f\"\"\"\nCREATE OR REPLACE TABLE {SURVEY_CLUSTER_OUTPUT} AS\nSELECT\n  s.SURVEY_ID,\n  s.REVIEW_TEXT,\n  s.KEYWORDS,\n  COALESCE(v.CLUSTER_LABELS, ARRAY_CONSTRUCT()) AS CLUSTER_LABELS\nFROM {SOURCE_TABLE} s\nLEFT JOIN V_SURVEY_LABELS v\n  ON s.SURVEY_ID = v.SURVEY_ID\n\"\"\").collect()\n\nprint(f\"âœ… Final output table written: {SURVEY_CLUSTER_OUTPUT}\")\n\n# Preview final output\nfinal_preview = session.table(SURVEY_CLUSTER_OUTPUT).limit(10).to_pandas()\nfinal_preview\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "954a3b30-b951-45cc-883b-cdb89de4b26d",
   "metadata": {
    "language": "sql",
    "name": "preview_labels"
   },
   "outputs": [],
   "source": "select * from SURVEY_CLUSTER_OUTPUT limit 10;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "42b74f7d-4f72-44f1-b66d-6ef8ede98fa5",
   "metadata": {
    "name": "AI_SQL",
    "collapsed": false
   },
   "source": "### USE AI to label surveys (alternate to ML method)"
  },
  {
   "cell_type": "code",
   "id": "8b6f1e55-9758-43db-b072-210dd17819b7",
   "metadata": {
    "language": "sql",
    "name": "AI_CLASSIFY"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE SURVEY_CLUSTER_OUTPUT_ALT AS \nSELECT *,\nAI_CLASSIFY(\n  review_text,\n  [\n    {'label': 'Product Feature', 'description': 'This review specifically mentions something about the features or a characteristic of a lululemon product, good or bad.'},\n    {'label': 'Fit and Sizing', 'description': 'This review mentions something about how the product fits, feels, or its size'},\n    {'label': 'Price Value', 'description': 'This review mentions something regarding the price or value of the product'},\n    {'label': 'Product Use', 'description': 'This review mentions something about how the product is being used like they are used for errands, fitness, yoga, etc'},\n    {'label': 'Suggestion', 'description': 'This review provides a suggestion from customers on how to improve the product or experience'}\n  ],\n  {\n    'task_description': 'Determine the appropriate topic labels for this lululemon product review',\n    'output_mode': 'multi',\n    'examples': [\n      {\n        'input': 'These exceeded my expectations! comfortable all day and great value.',\n        'labels': ['Fit and Sizing', 'Price Value'],\n        'explanation': 'the review mentions being comfortable which corresponds to Fit and Sizing and great value which corresponds to Price Value'\n      },\n      {\n        'input': 'They work well but sizing runs a bit small.',\n        'labels': ['Fit and Sizing', 'Price Value'],\n        'explanation': 'the review mentions being comfortable which corresponds to Fit and Sizing and great value which corresponds to Price Value'\n      }\n    ]\n  }) as AI_CLUSTER,\nFROM SURVEY_CLUSTER_OUTPUT;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "02bfc20d-6b54-47fb-952c-a67f20fd63f0",
   "metadata": {
    "language": "sql",
    "name": "preview_alt"
   },
   "outputs": [],
   "source": "select * from SURVEY_CLUSTER_OUTPUT_ALT limit 10;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9a5e8fba-14e3-4242-9bfc-5c593b8f05af",
   "metadata": {
    "language": "sql",
    "name": "FILTER"
   },
   "outputs": [],
   "source": "Select * from SURVEY_CLUSTER_OUTPUT_ALT\nWHERE AI_FILTER(CONCAT('This review contains a suggestion from the customer about how the product or service can be improved: ', REVIEW_TEXT));",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cc302c22-42da-4911-a4da-27a278fc7360",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "## Find sentiments of Surveys based on ASPECTS/Keyowrds and label"
  },
  {
   "cell_type": "code",
   "id": "9136ccb5-5803-4342-9218-21949473da94",
   "metadata": {
    "language": "sql",
    "name": "SENTIMENT",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE SENTIMENT_OUTPUT AS\nSELECT SURVEY_ID, REVIEW_TEXT, CLUSTER_LABELS,\nAI_SENTIMENT(REVIEW_TEXT, CLUSTER_LABELS) AS SENTIMENT\nFROM SURVEY_CLUSTER_OUTPUT_ALT;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "951161f6-b5d4-4c4b-885d-dc252b2917c3",
   "metadata": {
    "language": "sql",
    "name": "check_sentiment_data"
   },
   "outputs": [],
   "source": "SELECT * FROM SENTIMENT_OUTPUT limit 10;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7b5e00ff-2318-4714-9050-cbd4d2b3f04d",
   "metadata": {
    "language": "sql",
    "name": "EXP_VIEW_for_visualization"
   },
   "outputs": [],
   "source": "-- Explode sentiment categories, join to surveys, add normalized score + quarter keys\nCREATE OR REPLACE VIEW DEMO_DB.CRM.SENTIMENT_EXPLODED_VW AS\nSELECT\n  so.SURVEY_ID,\n  ps.PRODUCT_ID,\n  ps.PRODUCT_NAME,\n  DATE_TRUNC('QUARTER', ps.SURVEY_DATE)              AS QUARTER_START,\n  YEAR(ps.SURVEY_DATE)                                AS SURVEY_YEAR,\n  QUARTER(ps.SURVEY_DATE)                             AS SURVEY_QTR,\n  f.value:name::string                                AS CATEGORY,\n  LOWER(f.value:sentiment::string)                    AS SENTIMENT_LABEL,\n  CASE LOWER(f.value:sentiment::string)\n    WHEN 'positive' THEN 1.0::FLOAT\n    WHEN 'neutral'  THEN 0.5::FLOAT\n    WHEN 'unknown'  THEN 0.5::FLOAT\n    WHEN 'negative' THEN 0.0::FLOAT\n    ELSE NULL\n  END                                                 AS SENTIMENT_SCORE\nFROM DEMO_DB.CRM.SENTIMENT_OUTPUT so\nJOIN DEMO_DB.CRM.LULULEMON_PRODUCT_SURVEY ps\n  ON ps.SURVEY_ID = so.SURVEY_ID\n, LATERAL FLATTEN(input => so.SENTIMENT:categories) f\nWHERE ps.SURVEY_DATE IS NOT NULL AND\nps.review_text is not null AND\ncategory <> 'overall';\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "24bb92e4-e93a-406d-a2cc-6002912fc051",
   "metadata": {
    "language": "sql",
    "name": "SENTIMENT_EXPLODED_VW_data"
   },
   "outputs": [],
   "source": "select * from SENTIMENT_EXPLODED_VW limit 10;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "df03b4a5-e852-47fe-9cdf-4184afdb9042",
   "metadata": {
    "language": "python",
    "name": "data_viz",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "import streamlit as st\nst.title(\"Survey Sentiment\")\n\n# ---------------------------\n# Overall sentiment: single bar chart (avg score per product, descending)\n# ---------------------------\nst.subheader(\"Overall Sentiment Score by Product (0â€“1, higher is more positive)\")\n\noverall_df = session.sql(\"\"\"\n  SELECT\n    PRODUCT_NAME,\n    AVG(CAST(SENTIMENT_SCORE AS FLOAT)) AS AVG_SCORE\n  FROM DEMO_DB.CRM.SENTIMENT_EXPLODED_VW\n  GROUP BY PRODUCT_NAME\n  ORDER BY AVG_SCORE DESC\n\"\"\").to_pandas()\n\nif overall_df.empty:\n  st.info(\"No data available.\")\nelse:\n  by_product = (\n      overall_df\n      .set_index(\"PRODUCT_NAME\")[\"AVG_SCORE\"]\n      .sort_values(ascending=False)\n  )\n  \n  st.bar_chart(by_product)\n  st.dataframe(by_product, use_container_width=True)\n\n# ---------------------------\n# Product selector (single) â€” shown BELOW the overall chart\n# ---------------------------\nproducts_df = session.sql(\"\"\"\n  WITH base AS (\n    SELECT DISTINCT PRODUCT_ID, PRODUCT_NAME\n    FROM DEMO_DB.CRM.SENTIMENT_EXPLODED_VW\n  )\n  SELECT PRODUCT_ID, PRODUCT_NAME\n  FROM (\n    SELECT\n      PRODUCT_ID,\n      PRODUCT_NAME,\n      ROW_NUMBER() OVER (PARTITION BY PRODUCT_NAME ORDER BY PRODUCT_ID) AS RN\n    FROM base\n  )\n  WHERE RN = 1\n  ORDER BY PRODUCT_NAME\n\"\"\").to_pandas()\n\n\nproduct_display = products_df[\"PRODUCT_NAME\"].fillna(products_df[\"PRODUCT_ID\"])\nproduct_choice = st.selectbox(\n    \"Select a product\",\n    options=list(product_display),\n    index=0 if len(product_display) else None\n)\n\nif len(products_df):\n  selected_row = products_df.iloc[product_display[product_display == product_choice].index[0]]\n  selected_product_id = selected_row[\"PRODUCT_ID\"]\n  selected_product_name = selected_row[\"PRODUCT_NAME\"]\nelse:\n  selected_product_id = None\n  selected_product_name = None\n\n# ---------------------------\n# Product-specific: Top 5 positive labels (highest avg score)\n# ---------------------------\ncol1, col2 = st.columns(2)\n\nwith col1:\n  st.subheader(f\"Top 5 Positive Labels for {selected_product_name or '(no selection)'} (highest avg score)\")\n  if selected_product_id:\n    top_pos = session.sql(f\"\"\"\n      SELECT\n        CATEGORY,\n        AVG(CAST(SENTIMENT_SCORE AS FLOAT)) AS AVG_SCORE\n      FROM DEMO_DB.CRM.SENTIMENT_EXPLODED_VW\n      WHERE PRODUCT_ID = '{selected_product_id}'\n      GROUP BY CATEGORY\n      ORDER BY AVG_SCORE DESC NULLS LAST, CATEGORY\n      LIMIT 5\n    \"\"\").to_pandas()\n\n    # enforce order for the chart\n    top_pos = top_pos.sort_values(\"AVG_SCORE\", ascending=False)\n\n    if top_pos.empty:\n        st.info(\"No category data for this product.\")\n    else:\n        st.bar_chart(top_pos.set_index(\"CATEGORY\")[\"AVG_SCORE\"])\n        st.dataframe(top_pos, use_container_width=True)\n  else:\n    st.info(\"Select a product to see top positive labels.\")\n\n# ---------------------------\n# Product-specific: Top 5 negative labels (lowest avg score)\n# ---------------------------\nwith col2:\n  st.subheader(f\"Top 5 Negative Labels for {selected_product_name or '(no selection)'} (lowest avg score)\")\n  if selected_product_id:\n    top_neg = session.sql(f\"\"\"\n      SELECT\n        CATEGORY,\n        AVG(CAST(SENTIMENT_SCORE AS FLOAT)) AS AVG_SCORE\n      FROM DEMO_DB.CRM.SENTIMENT_EXPLODED_VW\n      WHERE PRODUCT_ID = '{selected_product_id}'\n      GROUP BY CATEGORY\n      ORDER BY AVG_SCORE ASC NULLS LAST, CATEGORY\n      LIMIT 5\n    \"\"\").to_pandas()\n\n    if top_neg.empty:\n      st.info(\"No category data for this product.\")\n    else:\n      st.bar_chart(top_neg.set_index(\"CATEGORY\")[\"AVG_SCORE\"])\n      st.dataframe(top_neg, use_container_width=True)\n  else:\n    st.info(\"Select a product to see top negative labels.\")\n\n# ---------------------------\n# Product-specific: Per-category trend over time (avg score by quarter)\n# ---------------------------\nst.subheader(f\"Per-Category Trend Over Time for {selected_product_name or '(no selection)'} (avg score per quarter)\")\n\nif selected_product_id:\n  trend_df = session.sql(f\"\"\"\n    WITH agg AS (\n      SELECT\n        CATEGORY,\n        QUARTER_START,\n        AVG(SENTIMENT_SCORE) AS AVG_SCORE\n      FROM DEMO_DB.CRM.SENTIMENT_EXPLODED_VW\n      WHERE PRODUCT_ID = '{selected_product_id}'\n      GROUP BY CATEGORY, QUARTER_START\n    ),\n    ranked AS (\n      -- Choose top 5 categories by number of time points to keep charts readable\n      SELECT\n        CATEGORY,\n        COUNT(*) AS PTS,\n        ROW_NUMBER() OVER (ORDER BY COUNT(*) DESC) AS RN\n      FROM agg\n      GROUP BY CATEGORY\n    )\n    SELECT a.CATEGORY, a.QUARTER_START, a.AVG_SCORE\n    FROM agg a\n    JOIN ranked r USING (CATEGORY)\n    WHERE r.RN <= 5\n    ORDER BY a.CATEGORY, a.QUARTER_START\n  \"\"\").to_pandas()\n\n  if trend_df.empty:\n    st.info(\"No category trend data for this product.\")\n  else:\n    cats = list(trend_df[\"CATEGORY\"].unique())\n    tabs = st.tabs(cats)\n    for tab, cat in zip(tabs, cats):\n      with tab:\n        df_cat = trend_df[trend_df[\"CATEGORY\"] == cat].set_index(\"QUARTER_START\").sort_index()\n        st.line_chart(df_cat[\"AVG_SCORE\"])\n        st.dataframe(df_cat.reset_index(), use_container_width=True)\nelse:\n  st.info(\"Select a product to see per-category trends.\")\n",
   "execution_count": null
  }
 ]
}