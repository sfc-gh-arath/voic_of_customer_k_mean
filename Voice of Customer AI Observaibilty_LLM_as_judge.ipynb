{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "3riktv4hbabagr45ws7v",
   "authorId": "330324676895",
   "authorName": "ADMIN",
   "authorEmail": "aswinee.rath@snowflake.com",
   "sessionId": "94a742f2-b425-4fbb-984f-8f1b31188839",
   "lastEditTime": 1760208011362
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "14305ae8-eee2-49c7-b555-9eb7c2ab8c52",
   "metadata": {
    "language": "python",
    "name": "pip_install_packages",
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "!pip install trulens==2.1.2 trulens-core==2.1.2 trulens-dashboard==2.1.2 trulens-feedback==2.1.2 trulens-otel-semconv==2.1.2 trulens_eval==2.1.2 trulens-connectors-snowflake==2.1.2 trulens-providers-cortex==2.1.2  \n!pip install snowflake==1.7.0 snowflake-ml-python==1.14.0 tabulate",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "import_packages",
    "codeCollapsed": false
   },
   "source": "import os, json, math, textwrap, ast\nimport numpy as np\nimport pandas as pd\nimport streamlit as st\nimport json\nimport streamlit as st\nimport time\nfrom functools import partial\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.cortex import complete\nfrom trulens.core import TruSession\nfrom trulens.connectors.snowflake import SnowflakeConnector\n#from trulens.apps.custom import instrument\nfrom trulens.apps.app import instrument\nfrom trulens.providers.cortex.provider import Cortex\nfrom trulens.core import Feedback, SnowflakeFeedback, Select\nfrom trulens.core.feedback import feedback as core_feedback\nfrom trulens.apps.custom import TruCustomApp\nfrom snowflake.core import Root\n\n\nimport hashlib\nfrom typing import List, Dict, Any, Optional, Tuple\nfrom datetime import datetime\n\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.functions import col\n\n\nfrom trulens.core import TruSession, Feedback, Select\nfrom trulens.apps.basic import TruBasicApp\nfrom trulens.providers.cortex import Cortex\nTRULENS_AVAILABLE = True\n\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4677024c-b3de-4e88-8b0a-96f5982193bb",
   "metadata": {
    "language": "python",
    "name": "create_a_sf_session"
   },
   "outputs": [],
   "source": "session = get_active_session()\nroot = Root(session)\n\nprint(\"✅ Snowpark session ready.\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4934b77e-6024-4a3c-ba28-2cd1faca9493",
   "metadata": {
    "language": "sql",
    "name": "pull_reviews"
   },
   "outputs": [],
   "source": "-- pre-uploaded synthetic data\nSELECT *\nFROM DEMO_DB.CRM.LULULEMON_PRODUCT_SURVEY\nLIMIT 10;\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d0c865e4-2fd2-4831-94bc-04f3286d3527",
   "metadata": {
    "language": "sql",
    "name": "complete_analyzed_data"
   },
   "outputs": [],
   "source": "select * from SURVEY_KEY_TOPICS limit 10;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79ae4935-65b9-4296-9212-2db2628f2c16",
   "metadata": {
    "language": "python",
    "name": "check_imports_are_done"
   },
   "outputs": [],
   "source": "TRULENS_AVAILABLE",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "df97f6a9-01d1-40d6-8f1c-21fe72c4eb3c",
   "metadata": {
    "language": "python",
    "name": "class_TruLensCortexEvaluator"
   },
   "outputs": [],
   "source": "\nclass TruLensCortexEvaluator:\n    \"\"\"\n    Evaluates Snowflake Cortex Complete prompts and outputs using TruLens.\n    \n    This class provides methods to:\n    1. Query Cortex Complete results from Snowflake\n    2. Evaluate relevance using TruLens feedback functions\n    3. Store evaluation results back to Snowflake\n    4. Generate observability reports\n    \"\"\"\n    \n    def __init__(\n        self,\n        session: Session,\n        provider: str = \"cortex\",\n        model: str = \"llama3.1-8b\", \n        cache_table: str = \"TRULENS_EVALUATION_CACHE\"\n    ):\n        \"\"\"\n        Initialize the evaluator.\n        \n        Args:\n            session: Snowflake Snowpark session\n            provider: LLM provider for evaluation ('cortex', 'openai', 'huggingface')\n            model: Model to use for evaluation\n                   - For 'cortex': 'llama3.1-8b', 'mistral-large', 'mixtral-8x7b', etc.\n                   - For 'openai': 'gpt-4', 'gpt-3.5-turbo', etc.\n            cache_table: Table name for caching evaluation results\n        \"\"\"\n        self.session = session\n        self.provider_name = provider\n        self.model = model\n        self.cache_table = cache_table\n        \n        # Initialize TruLens\n        if TRULENS_AVAILABLE:\n            self.tru = TruSession()\n            # Note: reset_database() is optional - only use if you want to clear previous data\n            # self.tru.reset_database()\n            \n            # Initialize feedback provider\n            if provider == \"cortex\":\n                if Cortex is None:\n                    raise ValueError(\n                        \"Cortex provider not available. \"\n                        \"Install with: pip install trulens-providers-cortex\"\n                    )\n                # Use Snowflake Cortex as the evaluation provider\n                self.provider = Cortex(\n                    snowflake_connection=self.session.connection,\n                    model_engine=model\n                )\n                print(f\"✓ Using Snowflake Cortex provider with model: {model}\")\n            else:\n                raise ValueError(\n                    f\"Unsupported provider: {provider}. \"\n                    f\"Choose from: 'cortex', 'openai', 'huggingface'\"\n                )\n            \n            # Setup feedback functions\n            self._setup_feedback_functions()\n        else:\n            print(\"TruLens not available. Using fallback Snowflake-native evaluation.\")\n            self.provider = None\n    \n    def _setup_feedback_functions(self):\n        \"\"\"Setup TruLens feedback functions for evaluation.\"\"\"\n        # Answer Relevance: How well does the output address the prompt?\n        if hasattr(self.provider, 'relevance'):\n            self.f_answer_relevance = Feedback(\n                self.provider.relevance,\n                name=\"Answer Relevance\"\n            ).on_input_output()\n        else:\n            self.f_answer_relevance = None\n        \n        # Sentiment: Is the response appropriate in tone?\n        if hasattr(self.provider, 'sentiment'):\n            self.f_sentiment = Feedback(\n                self.provider.sentiment,\n                name=\"Sentiment\"\n            ).on_output()\n        else:\n            self.f_sentiment = None\n        \n        # Conciseness: Is the response appropriately concise?\n        if hasattr(self.provider, 'conciseness'):\n            self.f_conciseness = Feedback(\n                self.provider.conciseness,\n                name=\"Conciseness\"\n            ).on_output()\n        else:\n            self.f_conciseness = None\n        \n        # Language match: Does output language match input? (not available in all providers)\n        if hasattr(self.provider, 'language_match'):\n            self.f_language_match = Feedback(\n                self.provider.language_match,\n                name=\"Language Match\"\n            ).on_input_output()\n        else:\n            self.f_language_match = None\n            \n        # Print available feedback functions\n        available = []\n        if self.f_answer_relevance: available.append(\"relevance\")\n        if self.f_sentiment: available.append(\"sentiment\")\n        if self.f_conciseness: available.append(\"conciseness\")\n        if self.f_language_match: available.append(\"language_match\")\n        print(f\"✓ Available feedback functions: {', '.join(available)}\")\n    \n    def _create_cache_table(self):\n        \"\"\"Create cache table if it doesn't exist.\"\"\"\n        create_sql = f\"\"\"\n        CREATE TABLE IF NOT EXISTS {self.cache_table} (\n            prompt_hash STRING,\n            output_hash STRING,\n            prompt TEXT,\n            output TEXT,\n            relevance_score FLOAT,\n            sentiment_score FLOAT,\n            conciseness_score FLOAT,\n            language_match_score FLOAT,\n            evaluation_timestamp TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),\n            metadata VARIANT,\n            CONSTRAINT unique_eval UNIQUE (prompt_hash, output_hash)\n        )\n        \"\"\"\n        self.session.sql(create_sql).collect()\n    \n    def _hash_text(self, text: str) -> str:\n        \"\"\"Generate hash for text.\"\"\"\n        return hashlib.md5(text.encode()).hexdigest()\n    \n    def _check_cache(self, prompt: str, output: str) -> Optional[Dict[str, float]]:\n        \"\"\"Check if evaluation exists in cache.\"\"\"\n        prompt_hash = self._hash_text(prompt)\n        output_hash = self._hash_text(output)\n        \n        try:\n            result = self.session.sql(f\"\"\"\n                SELECT \n                    relevance_score,\n                    sentiment_score,\n                    conciseness_score,\n                    language_match_score\n                FROM {self.cache_table}\n                WHERE prompt_hash = '{prompt_hash}'\n                AND output_hash = '{output_hash}'\n            \"\"\").collect()\n            \n            if result:\n                return {\n                    'relevance_score': result[0]['RELEVANCE_SCORE'],\n                    'sentiment_score': result[0]['SENTIMENT_SCORE'],\n                    'conciseness_score': result[0]['CONCISENESS_SCORE'],\n                    'language_match_score': result[0]['LANGUAGE_MATCH_SCORE']\n                }\n        except Exception:\n            pass\n        \n        return None\n    \n    def _save_to_cache(\n        self,\n        prompt: str,\n        output: str,\n        scores: Dict[str, float],\n        metadata: Optional[Dict] = None\n    ):\n        \"\"\"Save evaluation results to cache.\"\"\"\n        prompt_hash = self._hash_text(prompt)\n        output_hash = self._hash_text(output)\n        \n        # Escape single quotes\n        prompt_escaped = prompt.replace(\"'\", \"''\")\n        output_escaped = output.replace(\"'\", \"''\")\n        #metadata_json = json.dumps(metadata or {}).replace(\"'\", \"''\")\n        # Convert metadata to JSON string and escape quotes\n        metadata_json = json.dumps(metadata or {})\n        # Double escape: first for JSON, then for SQL string\n        metadata_escaped = metadata_json.replace(\"'\", \"''\")\n        \n        \n        insert_sql = f\"\"\"\n        INSERT INTO {self.cache_table} (\n            prompt_hash,\n            output_hash,\n            prompt,\n            output,\n            relevance_score,\n            sentiment_score,\n            conciseness_score,\n            language_match_score,\n            metadata\n        ) \n        select \n            '{prompt_hash}',\n            '{output_hash}',\n            '{prompt_escaped}',\n            '{output_escaped}',\n            {scores.get('relevance_score', 0)},\n            {scores.get('sentiment_score', 0)},\n            {scores.get('conciseness_score', 0)},\n            {scores.get('language_match_score', 0)},\n            parse_json('{metadata_escaped}')\n        \"\"\"\n        try:\n            self.session.sql(insert_sql).collect()\n        except Exception as e:\n            print(f\"Warning: Could not cache result: {e}\")\n    \n    def evaluate_single(\n        self,\n        prompt: str,\n        output: str,\n        context: Optional[str] = None,\n        use_cache: bool = True\n    ) -> Dict[str, float]:\n        \"\"\"\n        Evaluate a single prompt-output pair.\n        \n        Args:\n            prompt: The input prompt\n            output: The Cortex Complete output\n            context: Optional context (for RAG evaluation)\n            use_cache: Whether to use cached results\n            \n        Returns:\n            Dictionary of evaluation scores\n        \"\"\"\n        # Check cache first\n        if use_cache:\n            cached = self._check_cache(prompt, output)\n            if cached:\n                return cached\n        \n        if not TRULENS_AVAILABLE:\n            return self._fallback_evaluation(prompt, output)\n        \n        # Build list of available feedbacks\n        feedbacks = []\n        if self.f_answer_relevance:\n            feedbacks.append(self.f_answer_relevance)\n        if self.f_sentiment:\n            feedbacks.append(self.f_sentiment)\n        if self.f_conciseness:\n            feedbacks.append(self.f_conciseness)\n        if self.f_language_match:\n            feedbacks.append(self.f_language_match)\n        \n        if not feedbacks:\n            print(\"Warning: No feedback functions available, using fallback\")\n            return self._fallback_evaluation(prompt, output)\n        \n        # Initialize scores\n        scores = {\n            'relevance_score': 0.0,\n            'sentiment_score': 0.0,\n            'conciseness_score': 0.0,\n            'language_match_score': 0.0\n        }\n        \n        # Call provider methods directly instead of using TruBasicApp wrapper\n        # This is more reliable and avoids the \"No records found\" error\n        try:\n            # Evaluate relevance (prompt + output)\n            if hasattr(self.provider, 'relevance'):\n                try:\n                    relevance_result = self.provider.relevance(prompt, output)\n                    scores['relevance_score'] = float(relevance_result) if relevance_result is not None else 0.0\n                except Exception as e:\n                    print(f\"Warning: Relevance evaluation failed: {e}\")\n            \n            # Evaluate sentiment (output only)\n            if hasattr(self.provider, 'sentiment'):\n                try:\n                    sentiment_result = self.provider.sentiment(output)\n                    scores['sentiment_score'] = float(sentiment_result) if sentiment_result is not None else 0.0\n                except Exception as e:\n                    print(f\"Warning: Sentiment evaluation failed: {e}\")\n            \n            # Evaluate conciseness (output only)\n            if hasattr(self.provider, 'conciseness'):\n                try:\n                    conciseness_result = self.provider.conciseness(output)\n                    scores['conciseness_score'] = float(conciseness_result) if conciseness_result is not None else 0.0\n                except Exception as e:\n                    print(f\"Warning: Conciseness evaluation failed: {e}\")\n            \n            # Evaluate language match (prompt + output)\n            if hasattr(self.provider, 'language_match'):\n                try:\n                    language_result = self.provider.language_match(prompt, output)\n                    scores['language_match_score'] = float(language_result) if language_result is not None else 0.0\n                except Exception as e:\n                    print(f\"Warning: Language match evaluation failed: {e}\")\n                    \n        except Exception as e:\n            print(f\"Error during evaluation: {e}\")\n            print(\"Falling back to Snowflake Cortex evaluation\")\n            return self._fallback_evaluation(prompt, output)\n        \n        # Cache results with metadata\n        if use_cache:\n            metadata = {\n                'provider': self.provider_name,\n                'model': self.model,\n                'evaluation_method': 'direct_provider_call',\n                'timestamp': datetime.now().isoformat(),\n                'available_metrics': [\n                    'relevance' if hasattr(self.provider, 'relevance') else None,\n                    'sentiment' if hasattr(self.provider, 'sentiment') else None,\n                    'conciseness' if hasattr(self.provider, 'conciseness') else None,\n                    'language_match' if hasattr(self.provider, 'language_match') else None\n                ],\n                'context_provided': context is not None\n            }\n            # Remove None values from available_metrics\n            metadata['available_metrics'] = [m for m in metadata['available_metrics'] if m is not None]\n            \n            self._save_to_cache(prompt, output, scores, metadata)\n        \n        return scores\n    \n    def _fallback_evaluation(self, prompt: str, output: str) -> Dict[str, float]:\n        \"\"\"\n        Fallback evaluation using Snowflake Cortex when TruLens is not available.\n        \"\"\"\n        eval_prompt = f\"\"\"\n        Evaluate the following prompt and response pair on these criteria (0.0 to 1.0):\n        1. Relevance: How well does the response address the prompt?\n        2. Sentiment: Is the tone appropriate?\n        3. Conciseness: Is the response appropriately concise?\n        \n        Prompt: {prompt}\n        Response: {output}\n        \n        Return ONLY a JSON object with keys: relevance_score, sentiment_score, conciseness_score\n        Each value should be a float between 0.0 and 1.0.\n        \"\"\"\n        \n        try:\n            result = self.session.sql(f\"\"\"\n                SELECT SNOWFLAKE.CORTEX.COMPLETE(\n                    'mistral-large',\n                    '{eval_prompt.replace(\"'\", \"''\")}'\n                ) as evaluation\n            \"\"\").collect()\n            \n            eval_json = json.loads(result[0]['EVALUATION'])\n            # Ensure all expected keys exist\n            scores = {\n                'relevance_score': eval_json.get('relevance_score', 0.5),\n                'sentiment_score': eval_json.get('sentiment_score', 0.5),\n                'conciseness_score': eval_json.get('conciseness_score', 0.5),\n                'language_match_score': 0.0  # Not evaluated in fallback\n            }\n            return scores\n        except Exception as e:\n            print(f\"Fallback evaluation failed: {e}\")\n            return {\n                'relevance_score': 0.5,\n                'sentiment_score': 0.5,\n                'conciseness_score': 0.5,\n                'language_match_score': 0.0\n            }\n    \n    def evaluate_batch(\n        self,\n        data: pd.DataFrame,\n        prompt_column: str = 'prompt',\n        output_column: str = 'output',\n        id_column: str = 'id',\n        batch_size: int = 100\n    ) -> pd.DataFrame:\n        \"\"\"\n        Evaluate a batch of prompt-output pairs.\n        \n        Args:\n            data: DataFrame with prompts and outputs\n            prompt_column: Name of prompt column\n            output_column: Name of output column\n            id_column: Name of ID column\n            batch_size: Number of records to process at once\n            \n        Returns:\n            DataFrame with evaluation scores added\n        \"\"\"\n        results = []\n        \n        for idx, row in data.iterrows():\n            prompt = str(row[prompt_column])\n            output = str(row[output_column])\n            record_id = row[id_column] if id_column in row else idx\n            \n            scores = self.evaluate_single(prompt, output)\n            \n            result = {\n                id_column: record_id,\n                prompt_column: prompt,\n                output_column: output,\n                **scores\n            }\n            results.append(result)\n            \n            if (idx + 1) % batch_size == 0:\n                print(f\"Processed {idx + 1}/{len(data)} records\")\n        \n        return pd.DataFrame(results)\n    \n    def evaluate_snowflake_table(\n        self,\n        source_table: str,\n        prompt_column: str,\n        output_column: str,\n        result_table: str,\n        id_column: str = 'id',\n        sample_size: Optional[int] = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Evaluate Cortex Complete results from a Snowflake table.\n        \n        Args:\n            source_table: Source table name\n            prompt_column: Column containing prompts\n            output_column: Column containing Cortex outputs\n            result_table: Table to write results to\n            id_column: ID column name\n            sample_size: Optional sample size (for testing)\n            \n        Returns:\n            DataFrame with evaluation results\n        \"\"\"\n        # Create cache table\n        self._create_cache_table()\n        \n        # Query source data\n        query = f\"\"\"\n        SELECT \n            {id_column},\n            {prompt_column},\n            {output_column}\n        FROM {source_table}\n        \"\"\"\n        \n        if sample_size:\n            query += f\" LIMIT {sample_size}\"\n        \n        print(f\"Querying {source_table}...\")\n        df = self.session.sql(query).to_pandas()\n        print(f\"Retrieved {len(df)} records\")\n        \n        # Evaluate\n        print(\"Evaluating with TruLens...\")\n        results = self.evaluate_batch(\n            df,\n            prompt_column=prompt_column,\n            output_column=output_column,\n            id_column=id_column\n        )\n        \n        # Write results to Snowflake\n        print(f\"Writing results to {result_table}...\")\n        self.session.write_pandas(\n            results,\n            table_name=result_table,\n            auto_create_table=True,\n            overwrite=True\n        )\n        \n        return results\n    \n    def generate_report(self, results: pd.DataFrame) -> Dict[str, Any]:\n        \"\"\"\n        Generate summary report from evaluation results.\n        \n        Args:\n            results: DataFrame with evaluation scores\n            \n        Returns:\n            Dictionary with summary statistics\n        \"\"\"\n        report = {\n            'total_records': len(results),\n            'timestamp': datetime.now().isoformat(),\n            'metrics': {}\n        }\n        \n        for metric in ['relevance_score', 'sentiment_score', 'conciseness_score', 'language_match_score']:\n            if metric in results.columns:\n                report['metrics'][metric] = {\n                    'mean': float(results[metric].mean()),\n                    'median': float(results[metric].median()),\n                    'std': float(results[metric].std()),\n                    'min': float(results[metric].min()),\n                    'max': float(results[metric].max()),\n                    'below_threshold_count': int((results[metric] < 0.7).sum()),\n                    'below_threshold_pct': float((results[metric] < 0.7).sum() / len(results) * 100)\n                }\n        \n        return report\n    \n    def get_low_quality_records(\n        self,\n        results: pd.DataFrame,\n        threshold: float = 0.7,\n        metric: str = 'relevance_score'\n    ) -> pd.DataFrame:\n        \"\"\"\n        Get records with low quality scores.\n        \n        Args:\n            results: DataFrame with evaluation scores\n            threshold: Minimum acceptable score\n            metric: Metric to filter on\n            \n        Returns:\n            DataFrame with low quality records\n        \"\"\"\n        return results[results[metric] < threshold].sort_values(metric)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bc19678f-208b-496d-9abd-f50aea404220",
   "metadata": {
    "language": "python",
    "name": "sample_trulen_eval"
   },
   "outputs": [],
   "source": "# Initialize evaluator with Snowflake Cortex provider (recommended)\nevaluator = TruLensCortexEvaluator(\n    session=session,\n    provider=\"cortex\",  # Use Snowflake Cortex for evaluation\n    model=\"claude-4-sonnet\"  # or \"mistral-large\", \"mixtral-8x7b\", etc.\n)\n\n# Alternative: Use OpenAI provider\n# evaluator = TruLensCortexEvaluator(\n#     session=session,\n#     provider=\"openai\",\n#     model=\"gpt-4\"\n# )\n\n# Example 1: Evaluate a single prompt-output pair\nprint(\"\\n=== Example 1: Single Evaluation ===\")\nprompt = \"What is the return policy for online orders?\"\noutput = \"Our return policy allows returns within 30 days of purchase with original receipt.\"\n\nscores = evaluator.evaluate_single(prompt, output)\nprint(f\"Scores: {scores}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d044f0b6-fba0-483a-8033-d9f73a85fbae",
   "metadata": {
    "language": "sql",
    "name": "create_a_temp_SURVEY_KEY_TOPICS"
   },
   "outputs": [],
   "source": "create or replace transient table TEMP_SURVEY_KEY_TOPICS_PRMPT\nas\nSELECT survey_id, \nconcat ('Extract 1-6 concise product aspects from this retail review.\\n' ||\n'Rules: short phrases (2-4 words), copy from text if possible, no sentiment words, no duplicates.\\n' ||\n'Example: Great quality-husband loves them. Really comfy and true to size. -> [\"quality\", \"comfy\", \"size\"]\\n' ||\n'Return ONLY a JSON array of strings.\\n\\n' || REVIEW_TEXT) as PROMPT_GIVEN,\nKEYWORDS::string KEYWORDS\nFROM SURVEY_KEY_TOPICS\nwhere 1=1\nlimit 10\n;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c1433392-b367-4724-a239-51bccd56fe98",
   "metadata": {
    "language": "python",
    "name": "evaluate_SURVEY_KEY_TOPICS"
   },
   "outputs": [],
   "source": "# Evaluate from Snowflake table\nprint(\"\\n=== Evaluate Snowflake Table ===\")\n#a table with cortex complete promot and results\nresults = evaluator.evaluate_snowflake_table(\n    source_table='TEMP_SURVEY_KEY_TOPICS_PRMPT',\n    prompt_column='PROMPT_GIVEN',\n    output_column='KEYWORDS',\n    result_table='QA_SCORING_WITH_TRULENS',\n    id_column='SURVEY_ID',\n    sample_size=10  # Start with sample\n)\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "39da69f5-c3ea-4af2-aa2b-051e07eaff2d",
   "metadata": {
    "language": "python",
    "name": "see_results"
   },
   "outputs": [],
   "source": "results",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "43f9fa83-050c-4922-a23a-0cb65fa40ecf",
   "metadata": {
    "language": "sql",
    "name": "drop_transient_table"
   },
   "outputs": [],
   "source": "drop table TEMP_SURVEY_KEY_TOPICS_PRMPT;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b0dff18f-05fc-4c9b-8f55-d84d0b2bf66a",
   "metadata": {
    "language": "sql",
    "name": "see_stored_eval_score"
   },
   "outputs": [],
   "source": "select * from TRULENS_EVALUATION_CACHE;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "951161f6-b5d4-4c4b-885d-dc252b2917c3",
   "metadata": {
    "language": "sql",
    "name": "check_sentiment_data"
   },
   "outputs": [],
   "source": "SELECT * FROM SENTIMENT_OUTPUT limit 10;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1460cdc4-e59f-491d-8a61-8da1f93e8f93",
   "metadata": {
    "language": "sql",
    "name": "create_TEMP_SENTIMENT_OUTPUT_PRMPT"
   },
   "outputs": [],
   "source": "create or replace transient table TEMP_SENTIMENT_OUTPUT_PRMPT\nas\nSELECT survey_id \n,'provide overall sentiment and sentiment for the given review for classifcation labels deined in the lavel <review>' || review_text || '</review> <label>' || cluster_labels::string || '<label> . provide the output as JSON' as PROMPT_GIVEN\n,SENTIMENT::string SENTIMENT_OUTPUT\nFROM SENTIMENT_OUTPUT\nwhere 1=1\nlimit 10\n;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f10d9dbf-26d4-4341-89e5-9c23aebc2f81",
   "metadata": {
    "language": "python",
    "name": "eval_TEMP_SENTIMENT_OUTPUT_PRMPT"
   },
   "outputs": [],
   "source": "# Evaluate from Snowflake table\nprint(\"\\n=== Evaluate Snowflake Table ===\")\n#a table with cortex complete promot and results\nresults = evaluator.evaluate_snowflake_table(\n    source_table='TEMP_SENTIMENT_OUTPUT_PRMPT',\n    prompt_column='PROMPT_GIVEN',\n    output_column='SENTIMENT_OUTPUT',\n    result_table='SENTIMENT_OUTPUT_SCORING_WITH_TRULENS',\n    id_column='SURVEY_ID',\n    sample_size=10  # Start with sample\n)\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a3395773-85a2-4a85-a80f-89abf362ecd5",
   "metadata": {
    "language": "python",
    "name": "results_output"
   },
   "outputs": [],
   "source": "results",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "666ac46c-58f4-4f0f-9609-5abd5cf7addb",
   "metadata": {
    "language": "sql",
    "name": "output_SENTIMENT_OUTPUT_SCORING_WITH_TRULENS"
   },
   "outputs": [],
   "source": "select * from SENTIMENT_OUTPUT_SCORING_WITH_TRULENS limit 10;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "027a6363-754b-429e-8e34-58626810d4f0",
   "metadata": {
    "language": "sql",
    "name": "drop_TEMP_SENTIMENT_OUTPUT_PRMPT"
   },
   "outputs": [],
   "source": "drop table TEMP_SENTIMENT_OUTPUT_PRMPT;",
   "execution_count": null
  }
 ]
}